#+TITLE: Local search for SAT
#+LATEX_HEADER: \usepackage{algpseudocode}
#+OPTIONS: toc:nil

#+BEGIN_SRC R :session :exports none :results none
  require(ascii)
  require(ggplot2)
  require(ggfortify)
  library(survival)
#+END_SRC
* The problem
The decision version of a satisfiability problem is: Given a formula $F$ consisting of $m$ clauses
in which $n$ literals appear, does there exist an assignment (true or false) to the literal such
that the formula becomes true?  Here we assume that the formula is in conjunctive normal form (terms
in clauses are connected with disjunctions and all clauses are connected with conjunctions).  An
optimization version is to find an assignment of truth values to the literals which maximizes the
number of satisfied clauses---this is what the local search algorithm presented here will do.

* Neighborhoods and deltas

** Neighbourhoods
The neighbourhoods examined by the search are one-exchange neighbourhoods.  As booleans can only
take two different values, this means that a neighbour $s'$ of $s$ has the value of one literal
flipped; the neighbourhood function is:
\begin{equation*}
<v_1, v_2, \dots, v_k, v_{k+1}, \dots, v_n> \mapsto
\left\{<v_1, v_2, \dots, \neg v_k, v_{k+1}, \dots, v_n>\ \mid\ k \in \{1, 2, \dots, n\}\right\}
\end{equation*}
Where $<v_1, v_2, \dots, v_n>$ denotes an instantiation giving values to the literals.  The size of
the neighbourhood is $n-1$.

** Delta computation
Two static data structures are used to quickly find the clauses which are relevant for some literal
$j$:
\begin{align*}
C(x_j) &= \{c_i\ \mid\ x_j \text{ appears (not negated) in } c_i \} \\
\overline{C}(x_j) &= \{c_i\ \mid\ x_j \text{ appears negated in } c_i \}
\end{align*}
These two sets are implemented using vectors (index $i$ contains vector for clause $i$ which has
indices for literals) and they take up $O(n)$ space and take $O(km)$ time to initialize (where $k$
is the maximum number of literals in a clause).

Additionally, a dynamic structure is used to keep track of how many of the literals in the different
clauses are curently true (status criticality for each clause):
\begin{equation*}
s_i = \left| \left\{ x_j\ \mid\ x_j \in C_i \wedge x_j \right\} \cup
\left\{ x_j\ \mid\ \overline{x_j} \in C_i \wedge \overline{x_j} \right\} \right|
\end{equation*}
This structure is also implemented as a vector.  It takes up $O(m)$ space and it takes $O(km)$ time
to initialize.

Computing the delta (change in number of failed clauses) for a flip of a literal $l$ is done by
looking at all the clauses in which $l$ appears.  If, for some clause $C$, the value $s_C=0$, we
know for sure that flipping a literal in this clause will change it from unsatisfied to satisfied
(and so $\Delta$ is decremented).  If $s_C=1$, there are four cases to consider:
|                         | $l$ is true             | $l$ is false            |
|-------------------------+-------------------------+-------------------------|
| $C \in C(l)$            | $\Delta \gets \Delta+1$ | nothing happens         |
| $C \in \overline{C(l)}$ | nothing happens         | $\Delta \gets \Delta+1$ |
We know that if $l$ appears positively (non-negated) in $C$ and there is currently only one literal
satisfying $C$, then it is $l$ and flipping it would cause the clause to be unsatisfied.

Literals may appear in any clauses, so we may need to check all $m$ clauses meaning that the delta
computation takes $O(m)$ time.  When performing the flip, the value of the literal is negated
($O(1)$) and each clause the literal appears in is looked at ($O(m)$ again) and the counter $s_C$
for these clause is updated (incremented or decremented; this takes $O(1)$ for each of the $O(m)$
clauses).  Therefore, making a step takes $O(m)$ time.

* Search strategy 1: Min-conflict heuristic
** The algorithm
The search employed is min-conflict resolution combined with random walk (a crude implementation of
simulated annealing is also included in the source code, but min-conflict resolution is used by
default as it currently performs the best).  Starting from a random state, the search proceeds as
follows:

#+BEGIN_LaTeX
\begin{algorithmic}
  \For{$i = 1$ to maxIter}
    \State $C \gets$ uniformly randomly chosen currently unsatisfied clause
    \State $r \gets$ uniformly random real number $\in [0, 1)$
    \If{$r \leq p$}
      \State $l \gets$ uniformly randomly chosen literal appearing in $C$
    \Else
      \State $l \gets \text{argmin}_{l \in C} \Delta_{\text{flip}}(l)$
    \EndIf
    \State $V \gets V$ with $l = \neg l$
  \EndFor
\end{algorithmic}
#+END_LaTeX
Note that the actual implementation checks in the inner loop whether the cost is below some desired
goal (then the search can stop immediately) and it keeps track of the best seen solution so this is
returned even if the search moves away from it.

** Tuning the $p$ parameter
The choice of $p$ is central to the performance of the algorithm.  If $p$ is close to $0$, the
search can more easily get stuck in a local optimum (or rather, it may repeat a cycle of flips of
the same literals in the same clauses---this behaviour was observed in an early version without the
random literal flipping component).  On the other side, if $p$ is close to $1$, the search becomes
less focused on actually making overall improvements.  Nine values for $p$ from $0.1$ to $0.9$ have
been tested on a large uniform set of instances.[fn:instances]

As the algorithm is expected to solve the satisfiable test instances reasonably quickly, the tuning
will be to find the value for $p$ for which the solution is found the quickest.  Figure [[fig:sat-dist]]
shows the sample cumulative distribution over time for the different tested values---observe that
the extreme values $p<0.2$ and $p>0.7$ perform very badly.
#+HEADER: :height 4
#+BEGIN_SRC R :session :results graphics :file tune-p-sat.pdf :exports results
  sat <- read.table("../res/tune-p-satisfiable.csv", header=TRUE)
  sat$p <- factor(sat$p)
  sat$status <- ifelse(sat$cost==0,1,0)
  autoplot(survfit(Surv(time, status)~p, data=sat), fun="event", conf.int=FALSE)
#+END_SRC
#+NAME: fig:sat-dist
#+CAPTION: Sample cumulative distributions for the nine different "versions" of the algorithm showing the probability of each being done at a certain time.  These results are for the satisfiable instances.
#+RESULTS:
[[file:tune-p-sat.pdf]]

Similarly, for the unsatisfiable instances, the "original" version of the algorithm (with $p=0.2$)
found solutions for many instances with only one failed clause reasonably quickly, so on these
instances, the parameter values are compared on how quickly they find such a solution.  For these,
the extreme values also performed very badly.  Note, however, that it is not known by the author
whether such a solution exists---for some instances, all the different "versions" of the algorithm
failed to find a solution in 20 seconds.
#+HEADER: :height 4
#+BEGIN_SRC R :session :results graphics :file tune-p-unsat.pdf :exports results
  unsat <- read.table("../res/tune-p-unsatisfiable.csv", header=TRUE)
  unsat$p <- factor(unsat$p)
  unsat$status <- ifelse(unsat$cost<=1,1,0)
  autoplot(survfit(Surv(time, status)~p, data=unsat), fun="event", conf.int=FALSE)
#+END_SRC
#+NAME: fig:unsat-dist
#+CAPTION: Sample cumulative distributions for the nine different "versions" of the algorithm showing the probability of each being done at a certain time.  These results are for the unsatisfiable instances.
#+RESULTS:
[[file:tune-p-unsat.pdf]]

It is assumed that the different problem instances are somewhat comparable as they have the same
size and were reportedly generated in the same way.  To get rid of this assumption, however, one can
instead rank the algorithm "versions" on each instance and combine the results; figures [[fig:sat-box]]
and [[fig:unsat-box]] show the rank distributions for the two test classes.  Here, it is still obvious
that the extreme values perform the worst.  Additionally, it seems in both classes that values
$p=0.5$ and $p=0.6$ perform the best.

#+HEADER: :height 4
#+BEGIN_SRC R :session :results graphics :file tune-p-sat-rank.pdf :exports results
  T1 <- split(sat$time, sat$instance)
  T2 <- lapply(T1, rank, na.last="keep")
  T3 <- unsplit(T2, sat$instance)
  sat$rank <- T3
  rm(T1, T2, T3)

  ggplot(sat, aes(x=p, y=rank), fill=p) +
      geom_boxplot() + coord_flip()
#+END_SRC
#+NAME: fig:sat-box
#+CAPTION: Box plot showing the rank distribution (time to find satisfying solution) for the various tested values for the satisfiable instances.
#+RESULTS:
[[file:p3.pdf]]

#+HEADER: :height 4
#+BEGIN_SRC R :session :results graphics :file tune-p-unsat-rank.pdf :exports results
  T1 <- split(unsat$time, unsat$instance)
  T2 <- lapply(T1, rank, na.last="keep")
  T3 <- unsplit(T2, unsat$instance)
  unsat$rank <- T3
  rm(T1, T2, T3)
  ggplot(unsat, aes(x=p, y=rank), fill=p) +
      geom_boxplot() + coord_flip()
#+END_SRC
#+NAME: fig:unsat-box
#+CAPTION: Box plot showing the rank distribution (time to find solution with only one failed clause) for the various tested values for the unsatisfiable instances.
#+RESULTS:
[[file:p4.pdf]]

A different way to find the optimal value for p is to use the =race= package to enter different
values in a sort of competition.  The race setup tests each $p$ from a pool of candidates against a
pool of tests (the satisfiable instances were used) one at a time.  Any candidates which perform
statistically significantly worse (wrt. mean time to finish) are "killed".

In the first race with values $p \in \{0.2, 0.3, 0.4, \dots, 0.7\}$, the final selected candidate
was $0.5$ (two candidates were alive after all tests were run).

In a second race to fine-tune with values $p \in \{0.45, 0.46, 0.47, \dots, 0.65\}$, it seemed that
values from $0.5$ to $0.6$ were all pretty good; at the end of testing, 6 candidates were alive:
$0.52$, $0.53$, $0.56$, $0.57$, $0.58$ and $0.60$.

[fn:instances] The instances used all contain 250 literals and 1065 clauses.  They can be found at
[[http://www.cs.ubc.ca/~hoos/SATLIB/benchm.html]]

** Greedy construction heuristic
Instead of random initialization, one may consider whether it is worthwile to construct the initial
instantiation in a better way.  One such construction heuristic which was tried is the following:
for all literals, assign the to them the value (true or false) that would satisfy the most clauses
(which have not been satisfied by the assignment of other literals already).  To keep a random
element in the initialization, the order in which the literals are examined is randomised.  The time
spent doing this initialisation (at least the way it is implemented) is $O(n + m + nm)$ whereas
random initialisation takes $O(n)$ (both have still to construct the dynamic structures which also
takes time).

It turns out that this greedy initialisation is not much better than random.  The initial solution
generated by this greedy construction heuristic was typically around $86\%$ to $89\%$.  This is not
really an improvement over random initialisation; with 3-SAT formulas where literals are uniformly
randomly distributed, a random initialisation can be expected so satisfy $\frac{7}{8} = 87.5$ of all
clauses already.
#+HEADER: :height 4
#+BEGIN_SRC R :session :results graphics :file init-comp.pdf :exports results
  data <- read.table("../res/init-comp.csv", header=TRUE)
  data$it <- factor(data$it)
  ggplot(data, aes(x=initsat, colour=it)) + geom_density() +
      labs(x="percentage of clauses satisfied initially")
#+END_SRC
#+NAME: fig:init-comp
#+CAPTION: Comparison of the two initialisation strategies (0 is random, 1 is greedy).  Both seem pretty convincingly to be normally distributed with mean between $86\%$ and $88\%$.
#+RESULTS:
[[file:init-comp.pdf]]
#+END_SRC

More thorough testing on overall run-time confirms that there is hardly any
difference in results between these two initialisation strategies:
#+HEADER: :height 4
#+BEGIN_SRC R :session :results graphics :file greedy-init-sat.pdf :exports results
  data <- read.table("../res/test-greedy-init-sat.csv", header=TRUE)
  data$it <- factor(data$it)
  data$status <- ifelse(data$cost==0,1,0)
  autoplot(survfit(Surv(time, status)~it, data=data), fun="event", conf.int=FALSE)
#+END_SRC
#+NAME: fig:sat-greedy
#+CAPTION: Observed cumulative distributions over completion time (satisfiable instances) for initialisation strategy 0 (random) and 1 (greedy).
#+RESULTS:
[[file:greedy-init-sat.pdf]]

#+HEADER: :height 4
#+BEGIN_SRC R :session :results graphics :file greedy-init-unsat.pdf :exports results
  data <- read.table("../res/test-greedy-init-unsat.csv", header=TRUE)
  data$it <- factor(data$it)
  data$status <- ifelse(data$cost<=2,1,0)
  autoplot(survfit(Surv(time, status)~it, data=data), fun="event", conf.int=FALSE)
#+END_SRC
#+NAME: fig:unsat-greedy
#+CAPTION: Observed cumulative distributions over completion time (unsatisfiable instances) for initialisation strategy 0 (random) and 1 (greedy).  The search was stopped once a solution with two unsatisfied clauses was found.
#+RESULTS:

* Search strategy 2: Simulated annealing
** The algorithm
A different search strategy which was implemented and tested was simulated annealing.  The
implementation works as follows:
#+BEGIN_LaTeX
\begin{algorithmic}
  \State $T \gets T_{\text{initial}}$
  \Repeat
    \For{number of steps per temperature level}
      \State $l \gets$ uniformly randomly chosen literal
      \If{$\Delta_{\text{flip}}(l) \leq 0$}
        \State flip $l$
      \Else
        \State flip $l$ with probability $\phi(\Delta_{\text{flip}}(l), T)$
      \EndIf
    \EndFor
    \State $T \gets T \cdot \alpha$
  \Until{five temperatures have passed without improvement and the accepted ratio of worsening moves is less than $2\%$}
\end{algorithmic}
#+END_LaTeX
As the pseudocode implies, the annealing schedule used has a geometric cooling sequence---so here,
one must decide on parameters for the number of steps per temperature and the factor to multiply by
(in addition to the initial temperature).  Note that the actual implementation checks in the inner
loop whether the cost is below some desired goal (then the search can stop immediately) and it keeps
track of the best seen solution so this is returned even if the search moves away from it.

** Tuning the annealing schedule
As described, the annealing schedule has to be decided (initial temperature, the temperature
decrease function and the number of steps per temperature).  Ideally, one would specify that the
initial temperature should make it so that a given percentage of non-improving moves are accepted in
the beginning---this, however, requires advanced sampling and testing.  Instead, as the test
instances are similar, it is assumed that the ideal starting temperature is similar across the
tested instances.  Therefore, concrete numbers for the starting temperatures have simply been
compared using the race method.

In the race, the following parameters were experimented with:
| $\alpha$ | steps per temperature | initial temperature |
|----------+-----------------------+---------------------|
| $0.55$   | $30000$               | $10$                |
| $0.75$   | $60000$               | $20$                |
| $0.95$   |                       | $30$                |
|          |                       | $40$                |
|          |                       | $50$                |
Due to lack of further insight, all combinations of the above values for the three parameters
describing the annealing schedule were compared.  The "candidates" was run for up to five seconds
each on the satisfiable instances.  They were compared by their time to finish---plus the number of
violated clauses at the end multiplied by a constant larger than the maximal amount of time (so that
candidates not reaching a satisfying solution were punished heavily).

After 28 tasks, the race reported that the only candidate still alive was the one with
$\alpha=0.55$, $30000$ steps per temperature and an initial temperature of $20$; this is rather low
for all three parameters.  The results of the race are illustrated in figure [[fig:anneal-race]].
#+BEGIN_SRC R :session :results graphics :file annealing-race.pdf :exports results
  load("annealing-race.Rdata")
  data <- data.frame(apply(res$results, 2, {function (c) (sum(!is.na(c)))}))
  data$cand <- c(1:30)
  ggplot(data, aes(x=cand, y=lives)) + geom_bar(stat="identity") +
      coord_flip() +
      scale_x_discrete(=function(x){
          return(paste(x, "(", test$candidates[x,1],
                       ",", test$candidates[x,2],
                       ",", test$candidates[x,3],
                       ")", sep=""))},
          breaks=data$cand) +
      labs(x="Lifetime of candidate", y="Candidates")
#+END_SRC
#+NAME: fig:anneal-race
#+CAPTION: Plot showing how many tasks each candidate was run for (how long it survived).  On task 28, candidate 7 apparently got a score which finally showed it to be significantly better than candidate 1.  The numbers next to the candidates indicate $\alpha$, steps per temperature and initial temperature, respectively.
#+RESULTS:
[[file:annealing-race.pdf]]

* Conclusion
The (at least more) optimal values found for the various parameters for the two search strategies
have been as default.  To compare the two, they have been run on all the satisfiable instances and
the time to find a satisfying solution is compared (like during the tuning of the min-conflict
heuristic).  As figure [[fig:comp-strat]] indicates, the implemented simulated annealing strategy is not
currently competitive with the min-conflict heuristic.
#+HEADER: :height 4
#+BEGIN_SRC R :session :results graphics :file comp-strat.pdf :exports results
  data <- read.table("../res/comp-strats.csv", header=TRUE)
  data$ss <- factor(data$ss)
  data$status <- ifelse(data$cost==0,1,0)
  autoplot(survfit(Surv(time, status)~ss, data=data), fun="event", conf.int=FALSE)
#+END_SRC
#+NAME: fig:comp-strat
#+CAPTION: Comparing the estimated cumulative density functions for completion times (on satisfiable instances) for the two search strategies (0 being min-conflict heuristic and 1 being simulated annealing).
#+RESULTS:
